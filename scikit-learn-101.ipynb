{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0) Intro\n",
    "\n",
    "Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. \n",
    "\n",
    "It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. \n",
    "\n",
    "This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Prerequisites\n",
    "The reader must have basic knowledge about Machine Learning. He/she should also be aware about Python, NumPy, Scipy, Matplotlib. If you are new to any of these concepts, we recommend you take up tutorials concerning these topics, before you dig further into this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) What is Scikit-Learn (Sklearn)\n",
    "Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Installation\n",
    "If you already installed NumPy and Scipy, following are the two easiest ways to install scikit-learn −\n",
    "\n",
    "Using pip\n",
    "Following command can be used to install scikit-learn via pip −\n",
    "\n",
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Features\n",
    "Rather than focusing on loading, manipulating and summarising data, Scikit-learn library is focused on modeling the data. Some of the most popular groups of models provided by Sklearn are as follows −\n",
    "\n",
    "Supervised Learning algorithms − Almost all the popular supervised learning algorithms, like Linear Regression, Support Vector Machine (SVM), Decision Tree etc., are the part of scikit-learn.\n",
    "\n",
    "Unsupervised Learning algorithms − On the other hand, it also has all the popular unsupervised learning algorithms from clustering, factor analysis, PCA (Principal Component Analysis) to unsupervised neural networks.\n",
    "\n",
    "Clustering − This model is used for grouping unlabeled data.\n",
    "\n",
    "Cross Validation − It is used to check the accuracy of supervised models on unseen data.\n",
    "\n",
    "Dimensionality Reduction − It is used for reducing the number of attributes in data which can be further used for summarisation, visualisation and feature selection.\n",
    "\n",
    "Ensemble methods − As name suggest, it is used for combining the predictions of multiple supervised models.\n",
    "\n",
    "Feature extraction − It is used to extract the features from data to define the attributes in image and text data.\n",
    "\n",
    "Feature selection − It is used to identify useful attributes to create supervised models.\n",
    "\n",
    "Open Source − It is open source library and also commercially usable under BSD license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0) Scikit Learn - Modelling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Dataset Loading\n",
    "A collection of data is called dataset. It is having the following two components −\n",
    "\n",
    "Features − The variables of data are called its features. They are also known as predictors, inputs or attributes.\n",
    "\n",
    "Feature matrix − It is the collection of features, in case there are more than one.\n",
    "\n",
    "Feature Names − It is the list of all the names of the features.\n",
    "\n",
    "Response − It is the output variable that basically depends upon the feature variables. They are also known as target, label or output.\n",
    "\n",
    "Response Vector − It is used to represent response column. Generally, we have just one response column.\n",
    "\n",
    "Target Names − It represent the possible values taken by a response vector.\n",
    "\n",
    "Scikit-learn have few example datasets like iris and digits for classification and the Boston house prices for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "First 10 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(\"\\nFirst 10 rows of X:\\n\", X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Splitting the dataset\n",
    "To check the accuracy of our model, we can split the dataset into two pieces-a training set and a testing set. \n",
    "\n",
    "Use the training set to train the model and testing set to test the model. After that, we can evaluate how well our model did.\n",
    "\n",
    "### Example\n",
    "The following example will split the data into 70:30 ratio, i.e. 70% data will be used as training data and 30% will be used as testing data. The dataset is iris dataset as in above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4)\n",
      "(45, 4)\n",
      "(105,)\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size = 0.3, random_state = 1\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the example above, it uses train_test_split() function of scikit-learn to split the dataset. This function has the following arguments −\n",
    "\n",
    "X, y − Here, X is the feature matrix and y is the response vector, which need to be split.\n",
    "\n",
    "test_size − This represents the ratio of test data to the total given data. As in the above example, we are setting test_data = 0.3 for 150 rows of X. It will produce test data of 150*0.3 = 45 rows.\n",
    "\n",
    "random_size − It is used to guarantee that the split will always be the same. This is useful in the situations where you want reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Train the Model\n",
    "Next, we can use our dataset to train some prediction-model. \n",
    "\n",
    "As discussed, scikit-learn has wide range of Machine Learning (ML) algorithms which have a consistent interface for fitting, predicting accuracy, recall etc.\n",
    "\n",
    "### Example\n",
    "In the example below, we are going to use KNN (K nearest neighbors) classifier. \n",
    "\n",
    "Don’t go into the details of KNN algorithms, as there will be a separate chapter for that. This example is used to make you understand the implementation part only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9833333333333333\n",
      "Predictions: ['versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size = 0.4, random_state=1\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "classifier_knn.fit(X_train, y_train)\n",
    "y_pred = classifier_knn.predict(X_test)\n",
    "# Finding accuracy by comparing actual response values(y_test)with predicted response value(y_pred)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "# Providing sample data and the model will make prediction out of that data\n",
    "\n",
    "sample = [[5, 5, 3, 2], [2, 4, 3, 5]]\n",
    "preds = classifier_knn.predict(sample)\n",
    "pred_species = [iris.target_names[p] for p in preds] \n",
    "print(\"Predictions:\", pred_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Model Persistence\n",
    "Once you train the model, it is desirable that the model should be persist for future use so that we do not need to retrain it again and again. \n",
    "\n",
    "It can be done with the help of dump and load features of joblib package.\n",
    "\n",
    "Consider the example below in which we will be saving the above trained model (classifier_knn) for future use −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_classifier_knn.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(classifier_knn, 'iris_classifier_knn.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will save the model into file named iris_classifier_knn.joblib. \n",
    "\n",
    "Now, the object can be reloaded from the file with the help of following code −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.load('iris_classifier_knn.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) Preprocessing the Data\n",
    "As we are dealing with lots of data and that data is in raw form, before inputting that data to machine learning algorithms, we need to convert it into meaningful data. \n",
    "\n",
    "This process is called preprocessing the data. Scikit-learn has package named preprocessing for this purpose. The preprocessing package has the following techniques −"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1) Binarisation\n",
    "This preprocessing technique is used when we need to convert our numerical values into Boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "input_data = np.array([\n",
    "   [2.1, -1.9, 5.5],\n",
    "   [-1.5, 2.4, 3.5],\n",
    "   [0.5, -7.9, 5.6],\n",
    "   [5.9, 2.3, -5.8]]\n",
    ")\n",
    "data_binarized = preprocessing.Binarizer(threshold=0.5).transform(input_data)\n",
    "print(\"\\nBinarized data:\\n\", data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we used threshold value = 0.5 and that is why, all the values above 0.5 would be converted to 1, and all the values below 0.5 would be converted to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2) Mean Removal\n",
    "This technique is used to eliminate the mean from feature vector so that every feature centered on zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = [ 1.75  -1.275  2.2  ]\n",
      "Stddeviation =  [2.71431391 4.20022321 4.69414529]\n",
      "Mean_removed = [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
      "Stddeviation_removed = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "Input_data = np.array([\n",
    "   [2.1, -1.9, 5.5],\n",
    "   [-1.5, 2.4, 3.5],\n",
    "   [0.5, -7.9, 5.6],\n",
    "   [5.9, 2.3, -5.8]]\n",
    ")\n",
    "\n",
    "#displaying the mean and the standard deviation of the input data\n",
    "print(\"Mean =\", input_data.mean(axis=0))\n",
    "print(\"Stddeviation = \", input_data.std(axis=0))\n",
    "#Removing the mean and the standard deviation of the input data\n",
    "\n",
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"Mean_removed =\", data_scaled.mean(axis=0))\n",
    "print(\"Stddeviation_removed =\", data_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3) Scaling\n",
    "We use this preprocessing technique for scaling the feature vectors. Scaling of feature vectors is important, because the features should not be synthetically large or small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min max scaled data:\n",
      " [[0.48648649 0.58252427 0.99122807]\n",
      " [0.         1.         0.81578947]\n",
      " [0.27027027 0.         1.        ]\n",
      " [1.         0.99029126 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "Input_data = np.array(\n",
    "   [\n",
    "      [2.1, -1.9, 5.5],\n",
    "      [-1.5, 2.4, 3.5],\n",
    "      [0.5, -7.9, 5.6],\n",
    "      [5.9, 2.3, -5.8]\n",
    "   ]\n",
    ")\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "print (\"\\nMin max scaled data:\\n\", data_scaled_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4) Normalisation\n",
    "We use this preprocessing technique for modifying the feature vectors. Normalisation of feature vectors is necessary so that the feature vectors can be measured at common scale. There are two types of normalisation as follows −"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Normalisation\n",
    "It is also called Least Absolute Deviations. \n",
    "\n",
    "It modifies the value in such a manner that the sum of the absolute values remains always up to 1 in each row. \n",
    "\n",
    "Following example shows the implementation of L1 normalisation on input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 normalized data:\n",
      " [[ 0.22105263 -0.2         0.57894737]\n",
      " [-0.2027027   0.32432432  0.47297297]\n",
      " [ 0.03571429 -0.56428571  0.4       ]\n",
      " [ 0.42142857  0.16428571 -0.41428571]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "input_data = np.array(\n",
    "   [\n",
    "      [2.1, -1.9, 5.5],\n",
    "      [-1.5, 2.4, 3.5],\n",
    "      [0.5, -7.9, 5.6],\n",
    "      [5.9, 2.3, -5.8]\n",
    "   ]\n",
    ")\n",
    "data_normalized_l1 = preprocessing.normalize(input_data, norm='l1')\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Normalisation\n",
    "Also called Least Squares. \n",
    "\n",
    "It modifies the value in such a manner that the sum of the squares remains always up to 1 in each row. \n",
    "\n",
    "Following example shows the implementation of L2 normalisation on input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
